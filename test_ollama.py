#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Ollama Functionality
Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¸Ø§Ø¦Ù Ollama
"""

import sys
import subprocess
import time

def test_ollama_installation():
    """Test if Ollama is installed and accessible"""
    print("ğŸ”§ Ø§Ø®ØªØ¨Ø§Ø± ØªØ«Ø¨ÙŠØª Ollama...")
    
    try:
        # Check if ollama command is available
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        
        if result.returncode == 0:
            version = result.stdout.strip()
            print(f"âœ… Ollama Ù…Ø«Ø¨Øª: {version}")
            return True
        else:
            print("âŒ Ollama Ù…Ø«Ø¨Øª ÙˆÙ„ÙƒÙ† Ù„Ø§ ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­")
            return False
            
    except FileNotFoundError:
        print("âŒ Ollama ØºÙŠØ± Ù…Ø«Ø¨Øª")
        print("ğŸ’¡ Ù„ØªØ«Ø¨ÙŠØª Ollama:")
        print("   Windows: winget install Ollama.Ollama")
        print("   macOS: brew install ollama")
        print("   Linux: curl -fsSL https://ollama.ai/install.sh | sh")
        return False
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ollama: {e}")
        return False

def test_ollama_service():
    """Test if Ollama service is running"""
    print("\nğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø®Ø¯Ù…Ø© Ollama...")
    
    try:
        # Check if ollama service is responding
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… Ø®Ø¯Ù…Ø© Ollama ØªØ¹Ù…Ù„")
            return True
        else:
            print("âŒ Ø®Ø¯Ù…Ø© Ollama Ù„Ø§ ØªØ¹Ù…Ù„")
            print("ğŸ’¡ Ù„Ø¨Ø¯Ø¡ Ø®Ø¯Ù…Ø© Ollama:")
            print("   ollama serve")
            return False
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø®Ø¯Ù…Ø© Ollama: {e}")
        return False

def test_available_models():
    """Test available models"""
    print("\nğŸ¤– Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©...")
    
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        
        if result.returncode == 0:
            models_output = result.stdout
            print("âœ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©:")
            
            # Parse and display models
            lines = models_output.strip().split('\n')
            if len(lines) > 1:  # Skip header line
                for line in lines[1:]:
                    if line.strip():
                        parts = line.split()
                        if len(parts) >= 2:
                            model_name = parts[0]
                            model_size = parts[1] if len(parts) > 1 else "N/A"
                            print(f"   ğŸ“¦ {model_name} ({model_size})")
            
            # Check for required model
            if 'mistral:7b' in models_output:
                print("âœ… Ù†Ù…ÙˆØ°Ø¬ mistral:7b Ù…ØªØ§Ø­")
                return True
            else:
                print("âš ï¸ Ù†Ù…ÙˆØ°Ø¬ mistral:7b ØºÙŠØ± Ù…ØªØ§Ø­")
                print("ğŸ’¡ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
                print("   ollama pull mistral:7b")
                return False
        else:
            print("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
            return False
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")
        return False

def test_model_pull():
    """Test model pulling functionality"""
    print("\nğŸ“¥ Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    
    try:
        # Check if model already exists
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        
        if result.returncode == 0 and 'mistral:7b' in result.stdout:
            print("âœ… Ù†Ù…ÙˆØ°Ø¬ mistral:7b Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„")
            return True
        
        # Try to pull the model
        print("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ mistral:7b...")
        print("âš ï¸ Ù‡Ø°Ø§ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹...")
        
        pull_result = subprocess.run(['ollama', 'pull', 'mistral:7b'], 
                                   capture_output=True, text=True)
        
        if pull_result.returncode == 0:
            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ mistral:7b Ø¨Ù†Ø¬Ø§Ø­")
            return True
        else:
            print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
            print(f"Ø®Ø·Ø£: {pull_result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
        return False

def test_chat_ollama():
    """Test ChatOllama functionality"""
    print("\nğŸ’¬ Ø§Ø®ØªØ¨Ø§Ø± ChatOllama...")
    
    try:
        from langchain_ollama import ChatOllama
        
        # Test ChatOllama initialization
        llm = ChatOllama(
            model="mistral:7b",
            temperature=0.1,
            num_gpu_layers=0,  # Use CPU for testing
            num_thread=4
        )
        
        print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© ChatOllama")
        
        # Test simple query
        try:
            response = llm.invoke("Say hello in Arabic")
            print("âœ… Ù†Ø¬Ø­ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø³ÙŠØ·")
            print(f"   Ø§Ù„Ø±Ø¯: {response.content[:100]}...")
            return True
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {str(e)[:100]}")
            return False
            
    except ImportError:
        print("âŒ langchain_ollama ØºÙŠØ± Ù…Ø«Ø¨Øª")
        print("ğŸ’¡ Ù‚Ù… Ø¨ØªØ´ØºÙŠÙ„: pip install langchain_ollama")
        return False
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± ChatOllama: {e}")
        return False

def test_embeddings_ollama():
    """Test Ollama embeddings"""
    print("\nğŸ”¤ Ø§Ø®ØªØ¨Ø§Ø± Ollama Embeddings...")
    
    try:
        from langchain_ollama import OllamaEmbeddings
        
        # Test embeddings initialization
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ollama Embeddings")
        
        # Test embedding generation
        test_text = "Hello world"
        try:
            embedding = embeddings.embed_query(test_text)
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ† (Ø§Ù„Ø·ÙˆÙ„: {len(embedding)})")
            return True
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {str(e)[:100]}")
            return False
            
    except ImportError:
        print("âŒ langchain_ollama ØºÙŠØ± Ù…Ø«Ø¨Øª")
        return False
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Embeddings: {e}")
        return False

def test_ollama_performance():
    """Test Ollama performance"""
    print("\nâš¡ Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ollama...")
    
    try:
        from langchain_ollama import ChatOllama
        
        llm = ChatOllama(
            model="mistral:7b",
            temperature=0.1,
            num_gpu_layers=0,
            num_thread=4
        )
        
        # Test response time
        start_time = time.time()
        
        try:
            response = llm.invoke("What is 2+2?")
            end_time = time.time()
            
            response_time = end_time - start_time
            print(f"âœ… ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response_time:.2f} Ø«Ø§Ù†ÙŠØ©")
            
            if response_time < 10:
                print("âœ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø¨ÙˆÙ„")
                return True
            else:
                print("âš ï¸ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ø·ÙŠØ¡")
                return False
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡: {str(e)[:100]}")
            return False
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
        return False

def main():
    """Run all Ollama tests"""
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ollama...")
    
    tests = [
        ("ØªØ«Ø¨ÙŠØª Ollama", test_ollama_installation),
        ("Ø®Ø¯Ù…Ø© Ollama", test_ollama_service),
        ("Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©", test_available_models),
        ("ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬", test_model_pull),
        ("ChatOllama", test_chat_ollama),
        ("Ollama Embeddings", test_embeddings_ollama),
        ("Ø£Ø¯Ø§Ø¡ Ollama", test_ollama_performance)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ {test_name}: {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {passed}/{total} Ù†Ø¬Ø­")
    
    if passed == total:
        print("ğŸ‰ Ø¬Ù…ÙŠØ¹ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ollama Ù†Ø¬Ø­Øª!")
        print("âœ… Ollama Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹ Ø±ÙˆÙ†Ø§")
    else:
        print("âš ï¸ Ø¨Ø¹Ø¶ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ollama ÙØ´Ù„Øª")
        print("ğŸ’¡ Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø£Ø¹Ù„Ø§Ù‡")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)